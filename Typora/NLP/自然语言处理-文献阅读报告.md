**课程阅读报告要求**

1. 使用中文撰写，报告篇幅应在 5000 字以上，内容建议涵盖：
2. 背景介绍（论文所针对的 NLP 任务）；
3. 现有的方法及其局限性，论文方法的优越性；
4. 论文提出的方法（模型架构、训练策略等）；
5. 实验结果与分析（数据集、对比模型、数据分析等）；
6. 该方法的局限性，可能的拓展方向。





## 自然语言处理-文献阅读报告

### 背景介绍

神经机器翻译是最近提出的一种机器翻译方法。与传统的基于短语的翻译系统由许多单独调整的小子组件组成不同， 神经机器翻译尝试构建和训练单个大型神经网络来读取句子并输出正确的翻译。最近提出的神经机器翻译模型通常属于编码器-解码器架构，即将原句子编码为固定长度的向量，解码器从中生成翻译。

从概率的角度来看，翻译相当于找到一个目标句子 y，在给定源句子 x 的情况下最大化 y 的条件概率，所以我们可以使用神经机器翻译中的并行训练语料库拟合参数化模型来最大化句子对的条件概率。目前，有许多论文提出使用神经网络来直接学习这种条件分布，这种神经网络翻译方法通常由两个组件组成，第一个组件对原句子 x 进行编码，第二个句子对目标句子进行解码。这种神经网络翻译方法得到的结果已经十分不错。

### 现有方法

在编码器-解码器架构中，编码器将输入句子$x=(x_1,x_2,...,x_{T_x})$读取到向量$c^2$中。这一过程最常见的方法为使用循环神经网络（RNN），有：
$$
h_t=f(x_t,h_{t-1})\\
c=q({h_1,h_2,...,h_{T_x}})
\tag{1}
$$
其中$h_t∈R_n$是隐藏层t时刻的状态，c是从隐藏状态序列生成的向量，f和q是一些非线性函数。

而解码器的作用为在给定上下文向量c和所有先前预测的单词${y_1,y_2,...,y_{t-1}}$的情况下预测下一个单词$y_t$，即
$$
p(y)=\Pi^T_{t=1}p(y_t|{y_1,y_2,...,y_{t-1}},c)
\tag{2}
$$
其中$y=(y_1,y_2,...,y_{T_y})$。

使用RNN方法，每个状态都可以写为
$$
p(y_t|{y_1,y_2,...,y_{t-1}},c)=g(y_{t-1},s_t,c)
\tag{3}
$$
其中g是一个非线性，可能是多层的函数，输出$y_t$的概率。而$s_t$​为RNN的隐藏状态。

RNN的详细工作流程如下：

1. 输入表示：首先，输入序列会被转化为向量表示。在自然语言处理任务中，输入通常是文本序列，每个单词会被映射到一个向量表示。这个向量表示可以是预训练的词向量，也可以是通过神经网络学习到的。

2. 时间步处理：在RNN中，序列中的每个时间步都会被依次输入到RNN网络中。每个时间步的输入包括当前时间步的输入向量和上一个时间步的隐藏状态。RNN的核心思想是通过不断迭代更新隐藏状态，捕捉序列中的长期依赖关系。

3. 隐藏状态更新：在每个时间步，RNN都会根据当前输入和上一个时间步的隐藏状态计算出一个新的隐藏状态。这个隐藏状态会蕴含序列中之前时间步的信息，并且会传递给下一个时间步。

4. 输出计算：在RNN中，每个时间步都会有一个输出。这个输出可以用于进一步的处理或者预测。在文本生成任务中，RNN的输出可以被用来生成下一个单词；在时间序列预测任务中，RNN的输出可以被用来预测下一个时间步的数值。

5. 反向传播：在训练阶段，RNN会通过反向传播算法来更新网络中的参数，使得模型的预测结果更加准确。反向传播的过程中，会根据损失函数的值来计算梯度，并且根据梯度来更新网络中的权重和偏置。

总的来说，RNN通过不断迭代的方式来处理序列数据，通过更新隐藏状态来捕捉序列中的长期依赖关系。

### 现有方法局限性

现有方法中，一个潜在问题为神经网络需要将原句子的所有必要信息压缩成固定长度的向量，这可能会使神经网络难以处理长句子，尤其是比语料库中的句子更长的句子。研究表明，随着输入句子长度的增加，基本编码器-解码器的性能确实会迅速恶化。

### 论文方法优越性

而在论文方法中对编码器-解码器模型进行了扩展，通过学习联合对齐和翻译，在翻译生成每一个单词时，模型都会（软）搜索原句子中最相关信息中的一组位置，然后根据这些与原位置相关联的上下文向量以及所有先前生成的目标单词来预测当前目标单词。这种方法不会尝试将整个输入句子编码为单个固定长度的向量，而是将输入句子编码为向量序列，并在解码翻译时自适应地选择这些向量的子集。这使得神经翻译模型不必将源句子的所有信息压缩为固定长度的向量，所以能够更好地处理长句子。经定性分析，这种方法可行并且有效。



### 论文方法

#### 新的模型架构

定义公式(2)中的条件概率为
$$
p(y_i|y_1,...,y_{i-1},x) = g(y_{i-1},s_i,c_i)
\tag{4}
$$
其中$s_i$是时间$i$的RNN隐藏状态，公式为：
$$
s_i = f(s_{i-1},y_{i-1},c_i)
$$
注意这里的概率以每个目标词$y_i$的不同上下文向量$c_i$为条件，与之前给出的RNN方法不同。

![](NLP_img\figure1.png)

上下文向量$c_i$取决于编码器将输入句子映射到的注释序列($h_1,...,h_{T_x}$)。每个注释$h_i$包含有关整个输入序列的信息，重点关注输入序列第$i$个单词周围的部分。上下文向量$c_i$为这些注释$h_i$的加权和：
$$
c_i = \Sigma^{T_x}_{j=1}\alpha_{ij}h_j
\tag{5}
$$
(5)中每个注释$h_j$的权重$\alpha_{ij}$计算为：
$$
\alpha_{ij}=\frac{exp(e_{ij})}{\Sigma^{T_x}_{j=1}exp(e_{ik})}
\tag{6}
$$
其中
$$
e_{ij} = a(s_{i-1},h_j)
$$
是一个对齐模型，用于对位置$j$周围的输入与位置$i$的输出匹配程度进行评分。分数基于RNN隐藏状态$s_{i-1}$（公式(4)得到的$y_i$出现之前）和输入句子的第$j$个注释$h_j$。

将对齐模型$a$参数化为前馈神经网络，该网络与所提出系统的所有其他组件联合训练。与传统的机器翻译不同，对齐不被视为潜在变量。相反，对齐模型直接计算软对齐，这允许反向传播成本函数的梯度。该梯度可用于联合训练对齐模型以及整个翻译模型。

我们可以将所有注释的加权和的方法理解为计算预期注释，其中期望超过可能得对齐。令$\alpha_{ij}$为目标词$y_i$与源词$x_j$对齐或翻译自源词$x_j$的概率。然后第$i$个上下文向量$c_i$是所有概率为$\alpha_{ij}$的注释的预期注释。

概率$\alpha{ij}$或其相关能量$e_{ij}$反映了注释$h_j$相对于先前隐藏状态$s_{i-1}$在决定下一个状态$s_i$和生成$y_i$时的重要性。直观上，这在解码器中实现了注意力机制。解码器决定源语句中要注意的部分。通过让解码器具有注意力机制，我们减轻了编码器将源句子中的所有信息编码为固定长度向量的负担。通过这种新方法，信息可以分布在整个注释序列中，解码器可以相应地选择性地检索这些注释。

#### 模型需要使用的双向RNN

下面将构造用于注释序列的双向RNN。

通常的RNN如公式(1)，按照从第一个符号$x_1$到最后一个符号$x_{T_x}$的顺序读入输入序列$x$。然而，在刚刚提出的方案中，需要每个单词的注释不仅可以总结前面的单词，还可以总结后面的单词。因此需要使用双向RNN（BiRNN）。

BiRNN由前向和后向RNN组成。前向RNN$\overrightarrow{f}$按顺序读取输入序列（从$x_1$到$x_{T_x}$）并计算前向隐藏状态序列$(\overrightarrow{h_1},...,\overrightarrow{h_{T_x}})$。后向RNN$\overleftarrow{f}$以相反的顺序读取序列（从$x_{T_x}$到$x_1$）从而产生后向隐藏状态序列$(\overleftarrow{h_1},...,\overleftarrow{h_{T_x}})$。

通过连接前向隐藏状态和后向隐藏状态$h_j = [\overrightarrow{h_j^T};\overleftarrow{h_j^T}]^T$为每一个词$x_j$获得一个注释。通过这样的处理方式，注释$h_j$就包含了前面单词和后面单词的摘要。由于RNN倾向于更好地表示最近的输入，因此注释$h_j$将集中在$x_j$周围的单词上。解码器和对齐模型随后使用该注释序列来计算上下文向量（公式(5)-(6)）。

$Fig.1$为模型示意图。

### 实验结果与分析（数据集、对比模型、数据分析等）

#### 数据集

这篇论文使用news-test-2012和news-test-2012来制作来训练集合，之后使用缩小后的WMT'14作为测试集来评估模型。所选择的WMT‘14为包含Europarl（61M），新闻评论（5.5M），UN(421M)以及两个语料库中的850M单词，之后使用Axelrod等人的数据选择方法将组合语料库的大小减少到348M个单词，并且不使用任何单语数据来预训练编码器。这样一来，该测试集中包含训练模型中不存在的3003个句子。

#### 对比模型

论文中训练了两种模型，一个是已有的RNN编码器-解码器模型，另一个是论文中所提出的模型，称为RNNsearch模型。之后对每个模型进行两次训练，第一次使用长度最多为30个单词的句子训练，称为RNNencdec-30和RNNsearch-30；第二次使用长度最多为50个单词的句子训练，称为RNNencdec-50和RNNsearch-50，共计4个模型。

其中，RNNencdec 的编码器和解码器各有 1000 个隐藏单元。RNNsearch 的编码器由前向和后向循环神经网络组成，每个网络都有 1000 个隐藏单元共计2000个隐藏单元。而它的解码器和RNNencdec一样有 1000 个隐藏单元。在这两种情况下，实验都使用具有单个 maxout 隐藏层的多层网络来计算每个目标单词的条件概率。

关于模型训练，实验使用小批量随机梯度下降 (SGD) 算法和 Adadelta来训练每个模型。每个 SGD 更新方向都是使用 80 个句子的小批量计算的。每个模型都进行了大约 5 天的训练。模型训练完成后，实验使用集束搜索来找到近似最大化条件概率的翻译，从神经网络翻译机器翻译模型生成翻译。

#### 数据分析

##### 定量分析

表一中列出了以BLUE分数（取值范围为 0～1，分数越接近1，说明翻译的质量越高）衡量的翻译性能。从表中可知，在所有情况下论文提出的RNNsearch方法都优于传统的RNNencdec方法。进一步来说，当只考虑由已知单词组成的句子时，RNNsearch搜索的性能与传统的基于短语的翻译系统（Moses）一样高（Moses使用的是单独语料库，更加说明RNNsearch搜索性能之高）。

图二中显示了测试集上生成的翻译相对与句子长度的BLUE分数。从图中可以看出，随着句子长度的增加，RNNencdec的性能急剧下降。而对于RNNsearch来说，当训练句子长度为30时，在面对长句子翻译时性能也会有所下降，但当训练句子长度为50时，即使翻译句子长度为50或者更长，性能也不会下降。并且RNNsearch-30甚至优于RNNencdec-50,这一事实进一步证实了所提出的模型相对于基本编码器-解码器的优越性。

![](NLP_img\chart1.png)

![](NLP_img\figure2.png)

##### 定性分析

###### 对齐

实验通过可视化等式中的注释权重$α_{ij}$来直观地检查生成地翻译中的单词与原句子之间的（软）对齐。如图三所示，矩阵中的每一行表示与注释相关的权重，从中可以直观地看出在生成目标单词时原句子中的哪些位置更加重要。

从图中的对齐可以看出英语和法语之间的单词对齐基本上是单调的，每个对角线都有很强的权重。然而也存在很多重要的非单调的排列。法语和英语中形容词和名词的顺序通常不同，图3(a)中我们可以看到模型正确地将短语语 [European Economic Area] 翻译为 [zoneéconomique européen]。本次实验提出的RNNsearch 能够正确地将 [zone] 与 [Area] 对齐，跳过这两个单词 （[European] 和 [Economic]）之后回看一个单词以完成整个短语 [zone économique européenne]。

软对齐有两个好处。其一为翻译强度相比与硬对齐更高。如图三中3(d)中的短语 [the man]，它被翻译成 [l' homme]。任何硬对齐都会将 [the] 映射到 [l']， 将 [man] 映射到 [homme]。这对翻译没有帮助，因为必须考虑[the]后面的单词来确定是否应该翻译为[le]、[la]、[les]或[l']。我们的软对齐通过让模型同时查看 [the] 和 [man] 自然地解决了这个问题，在这个例子中，我们看到模型能够正确地将 [the] 翻译为 [l']。对于图3中的其他矩阵，我们同样可以发现这一点。

###### 长句子

从图2中可以清楚地看出，所提出的模型（RNNsearch）在翻译长句子方面比传统模型（RNNencdec）要好得多。这可能是因为 RNN 搜索不需要将长句子完美地编码为固定长度的向量，而只需要准确地编码输入句子中围绕特定单词的部分。考虑测试集中的一个句子：

`This kind of experience is part of Disney's efforts to "extend the lifetime of its series and build new relationships with audiences via digital platforms that are becoming ever more important," he added.`

RNNencdec 在生成大约30个单词后开始偏离源句子的实际含义，之后翻译质量下降并出现基本错误。但是RNNsearch可以正确地翻译该句子。结合定量分析，定性分析证实了RNNsearch能够比RNNencdec更可靠地翻译长句子。

![](NLP_img\figure3.png)